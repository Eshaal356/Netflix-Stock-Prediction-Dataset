# -*- coding: utf-8 -*-
"""Netflix_Stock_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kCb6Q4_QFbH0EO7GjXPa1WP4PSZaJiix

---
**In this project, historical daily stock data of Netflix (NFLX) is used to analyze market behavior and predict future closing prices using machine learning models.**

---

#**ðŸ“Œ STEP 1: Importing Required Libraries**
**ðŸ“˜ Why this step is important**

Before working with data, models, or visualizations, we must import all required libraries.
This shows planning, avoids repeated imports, and keeps the notebook clean.

---
"""

# ================================
# Core Data Handling Libraries
# ================================
import pandas as pd
import numpy as np
# ================================
# Data Visualization Library
# ================================
import matplotlib.pyplot as plt
# ================================
# Machine Learning (Baseline Model)
# ================================
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# ================================
# Deep Learning Libraries (LSTM)
# ================================
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
# ================================
# Utilities
# ================================
import warnings
warnings.filterwarnings("ignore")

"""#**Explanation:**

---
*   pandas (pd) â†’ reading CSV files, DataFrames, data cleaning.
*   numpy (np) â†’ numerical operations, arrays, reshaping data for models.
*   matplotlib â†’ plotting stock prices, trends, predictions
*   Industry-standard for analytical visualization
*   Used for baseline regression model
*   Metrics help evaluate accuracy and error magnitude
*   Sequential â†’ stack neural network layers linearly
*   LSTM â†’ captures long-term temporal patterns in stock prices
*   Dense â†’ final prediction layer
*   Dropout â†’ prevents overfitting
*   EarlyStopping â†’ stops training when performance stops improving
*   Adam â†’ efficient optimizer for time-series data
---

#**ðŸ”¹ STEP 2: Data Uploading & Initial Exploration**
---
"""

df = pd.read_csv('/content/NFLX.csv')
df

"""**Explanation:**

---

* pd.read_csv() reads the CSV file into memory

* The data is stored in a DataFrame, which is a table-like structure

* df is a conventional variable name used across data science projects

This confirms the dataset is successfully loaded.

---

**ðŸ“Œ Step 2.1: Display First Few Rows of the Dataset**

---
"""

df.head()

"""**Explanation:**

---
* Displays the first 5 rows of the dataset

* Helps understand:

     * Column names

     * Data format

     * Whether values are realistic

* This is always the first inspection step in professional data analysis
---

**ðŸ“Œ Step 2.2: Display Last Few Rows of the Dataset**

---
"""

df.tail()

"""**Explanation:**

---

* Displays the last 5 rows

* Confirms:

  * Dataset end date

  * No truncation or missing records at the end

---

**ðŸ“Œ Step 2.3: Check Dataset Shape (Rows & Columns)**

---
"""

df.shape

"""**Explanation:**

---

* Returns a tuple:
(number of rows, number of columns)

* Helps understand:

  * Dataset size

  * Amount of historical data available

---

**ðŸ“Œ Step 2.4: Display Column Names**

---
"""

df.columns

"""**Explanation:**

---

* Lists all feature names in the dataset

* Confirms availability of:

  * Date

  * Open

  * High

  * Low

  * Close

  * Adj Close

  * Volume

* This step helps decide which features are relevant for prediction.

---

**ðŸ“Œ Step 2.5: Dataset Information (VERY IMPORTANT)**

---
"""

df.info()

"""**Explanation:**

---

* This command provides:

  * Total number of entries

  * Data types of each column

  * Non-null counts

  * Memory usage

* Why this is critical:

  * Ensures there are no missing values

  * Confirms numerical columns are correctly formatted

  * Shows the dataset is ready for analysis and modeling

---

**ðŸ“Œ Step 2.6: Statistical Summary of the Dataset**

---
"""

df.describe()

"""**Explanation:**

---

* Generates summary statistics for numerical columns:

  * Mean

  * Standard deviation

  * Minimum & maximum values

* Helps understand:

  * Price volatility

  * Volume distribution

  * Market behavior

 ---

**ðŸ“Œ Step 2.7: Check for Missing Values**

---
"""

df.isnull().sum()

"""**Explanation:**

---

* Checks each column for missing (null) values

* Clean data is critical for machine learning

* If null values existed, we would handle them here

---

**ðŸ“Œ Step 2.8: Convert Date Column to Datetime & Set Index**

---
"""

df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)

"""**Explanation:**
---

* Converts Date from string format to datetime

* Setting Date as index:

  * Enables time-series analysis

  * Required for plotting trends

  * Necessary for LSTM modeling later

---

#**ðŸ”¹ STEP 3: Data Cleaning & Feature Engineering**
---

**ðŸ“Œ Step 3.1: Confirm Dataset Is Sorted by Date**

---
"""

df = df.sort_index()

"""**Explanation:**

---
* Stock data must be in chronological order

* Time-series models (especially LSTM) depend on correct sequencing

* Sorting by index (Date) ensures no future data leaks into past data

---

**ðŸ“Œ Step 3.2: Create Daily Price Change**

---
"""

df['Daily_Price_Change'] = df['Close'] - df['Open']

"""**Explanation:**

---
* Measures intra-day movement

* Positive values â†’ bullish day

* Negative values â†’ bearish day

* Helps understand volatility and trader behavior

---

**ðŸ“Œ Step 3.3: Create Daily Percentage Return**

---
"""

df['Daily_Return'] = df['Close'].pct_change()

"""**Explanation:**

---
* Percentage change is more meaningful than raw price

* Used heavily in financial analytics

* Captures relative market movement

---

**ðŸ“Œ Step 3.4: Create Moving Averages (Trend Indicators)**

---
"""

df['MA_20'] = df['Close'].rolling(window=20).mean()
df['MA_50'] = df['Close'].rolling(window=50).mean()

"""**Explanation:**

---

* 20-Day MA â†’ short-term trend

* 50-Day MA â†’ long-term trend

* Smooths noise and highlights direction

* Used by professional traders

---

**ðŸ“Œ Step 3.5: Handle Missing Values from Rolling Calculations**

---
"""

df.dropna(inplace=True)

"""**Explanation:**

---

* Moving averages create NaN values initially

* Removing them ensures clean data for modeling

* Prevents training errors later

---

#**ðŸ”¹ STEP 4: Exploratory Data Analysis (EDA)**

---

**ðŸ“Œ Step 4.1: Closing Price Over Time**

---
"""

plt.figure()
plt.plot(df.index, df['Close'])
plt.title("Netflix Closing Price Over Time")
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.show()

"""**Explanation:**

---

* Shows overall growth and decline phases

* Helps visually identify:

  * Trends

  * Crashes

  * Recovery periods

---

**ðŸ“Œ Step 4.2: Trading Volume Over Time**

---
"""

plt.figure()
plt.plot(df.index, df['Volume'])
plt.title("Netflix Trading Volume Over Time")
plt.xlabel("Date")
plt.ylabel("Volume")
plt.show()

"""**Explanation:**

---

* High volume = strong market interest

* Volume spikes often precede price reversals

* Indicates institutional activity

---

**ðŸ“Œ Step 4.3: Price with Moving Averages (4D Insight)**

---
"""

plt.figure(figsize=(14,6))
plt.plot(df.index, df['Close'], label='Close Price')
plt.plot(df.index, df['MA_20'], label='20-Day MA')
plt.plot(df.index, df['MA_50'], label='50-Day MA')
plt.title("Netflix Price Trend with Moving Averages")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.show()

"""**Explanation:**

---

This single plot represents 4 dimensions:

  * Time

  * Price

  * Short-term trend

  * Long-term trend

Crossovers indicate trend reversals.

---

#**ðŸ”¹ STEP 5: Prepare Data for Machine Learning (Linear Regression)**

---
"""

X = df[['Open', 'High', 'Low', 'Volume', 'MA_20', 'MA_50']]
y = df['Close']

"""**Explanation:**

---

* Uses multiple market indicators as inputs

* Close price is the prediction target

* Demonstrates multivariate modeling

---

**ðŸ“Œ Step 5.2: Train-Test Split (Time-Aware)**

---
"""

split_ratio = int(len(df) * 0.8)

X_train = X[:split_ratio]
X_test = X[split_ratio:]

y_train = y[:split_ratio]
y_test = y[split_ratio:]

"""**Explanation:**

---

* No random shuffling (important!)

* Mimics real-world forecasting

* Past data predicts future data

---

**ðŸ“Œ Step 5.3: Train Linear Regression Model**

---
"""

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

"""**Explanation:**

---
* Linear Regression is used as a baseline

* Fast, interpretable, reliable for stable markets

---

**ðŸ“Œ Step 5.4: Make Predictions**

---
"""

lr_predictions = lr_model.predict(X_test)

"""**ðŸ“Œ Step 5.5: Evaluate Model Performance**

---
"""

mae = mean_absolute_error(y_test, lr_predictions)
rmse = np.sqrt(mean_squared_error(y_test, lr_predictions))
r2 = r2_score(y_test, lr_predictions)

mae, rmse, r2

"""**Explanation:**

---

* MAE â†’ average prediction error

* RMSE â†’ penalizes large errors

* RÂ² â†’ how well the model explains price variation

---

**ðŸ“Œ Step 5.6: Actual vs Predicted Plot**

---
"""

plt.figure(figsize=(14,6))
plt.plot(y_test.index, y_test, label='Actual Price')
plt.plot(y_test.index, lr_predictions, label='Predicted Price')
plt.title("Actual vs Predicted Closing Price (Linear Regression)")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.show()

"""**Explanation:**

---

* Visual comparison of model performance

* Shows prediction accuracy and lag

---
"""

residuals = y_test - lr_predictions

plt.figure(figsize=(12,5))
plt.plot(residuals)
plt.axhline(0)
plt.title("Residual Error Over Time")
plt.xlabel("Time")
plt.ylabel("Prediction Error")
plt.show()

"""The increase in residual magnitude during late periods corresponds to heightened market volatility, highlighting the limitations of linear models during abrupt price movements.

**Step 5.7: Prediction with Confidence Interval**

----
"""

error_band = residuals.std()

plt.figure(figsize=(14,6))
plt.plot(y_test.index, y_test, label='Actual Price')
plt.plot(y_test.index, lr_predictions, label='Predicted Price')

plt.fill_between(
    y_test.index,
    lr_predictions - error_band,
    lr_predictions + error_band,
    alpha=0.2,
    label='Confidence Band'
)

plt.title("Prediction with Confidence Interval")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.show()

"""#**ðŸ”¹ STEP 6: LSTM Model (Advanced)**

---

**ðŸ“Œ Step 6.1: Scale Data (LSTM Requirement)**

---
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_close = scaler.fit_transform(df[['Close']])

"""**Explanation:**

---

* LSTM performs better with scaled data

* Normalizes values between 0 and 1

---

**ðŸ“Œ Step 6.2: Create Sequences for LSTM**

---
"""

def create_sequences(data, seq_length=60):
    X, y = [], []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

X_lstm, y_lstm = create_sequences(scaled_close)

"""**Explanation:**

---

* Uses last 60 days to predict next day

* Mimics how humans analyze trends

---

**ðŸ“Œ Step 6.3: Reshape Data for LSTM**

---
"""

X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))

"""**ðŸ“Œ Step 6.4: Train-Test Split for LSTM**

---
"""

split = int(len(X_lstm) * 0.8)

X_train_lstm = X_lstm[:split]
X_test_lstm = X_lstm[split:]

y_train_lstm = y_lstm[:split]
y_test_lstm = y_lstm[split:]

"""**ðŸ“Œ Step 6.5: Build LSTM Model**

---
"""

model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(50))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

"""**Explanation:**

---

* Two LSTM layers capture deep patterns

* Dropout prevents overfitting

* Dense layer outputs prediction

---

**ðŸ“Œ Step 6.6: Train LSTM Model**

---
"""

model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32)

"""**ðŸ“Œ Step 6.7: LSTM Predictions**

---
"""

lstm_predictions = model.predict(X_test_lstm)
lstm_predictions = scaler.inverse_transform(lstm_predictions)

"""**ðŸ“Œ Step 6.8: LSTM Prediction Plot**

---
"""

plt.figure(figsize=(14,6))
plt.plot(actual_prices, label='Actual Price')
plt.plot(lstm_predictions, label='LSTM Predicted Price')
plt.title("LSTM Stock Price Prediction")
plt.xlabel("Time")
plt.ylabel("Price")
plt.legend()
plt.show()

"""#**âœ… Final Conclusion**

---
In this project, historical daily stock price data of Netflix (NFLX) was analyzed to understand market behavior and predict future closing prices using both traditional machine learning and deep learning approaches.

---

The data was thoroughly explored and preprocessed through structured inspection, statistical analysis, and feature engineering. Key financial indicators such as moving averages (20-day and 50-day) and daily returns were created to capture short-term momentum and long-term trends. Exploratory data analysis revealed clear trend phases, volatility periods, and volume-driven price movements.

---

A Linear Regression model was implemented as a baseline predictor using multiple market features. The model achieved strong performance metrics (low MAE and RMSE with a high RÂ² score), indicating effective learning of price trends during stable market conditions. Residual error analysis showed that prediction errors were centered around zero, confirming the absence of systematic bias. Larger residuals during high-volatility periods highlighted the known limitations of linear models in rapidly changing markets.

---

To address temporal dependencies and non-linear patterns, a Long Short-Term Memory (LSTM) neural network was developed. The LSTM model demonstrated improved trend-following behavior by capturing long-term sequential relationships in stock prices. Its smoother predictions reflected an ability to filter short-term noise and focus on underlying market direction, making it suitable for time-series forecasting.

---

Overall, this project demonstrates an end-to-end data science workflow, including data preprocessing, visualization, feature engineering, model development, evaluation, and interpretation. It highlights practical understanding of financial time-series modeling, model limitations, and real-world market behavior. The approach is scalable and can be extended to real-time stock analysis, risk assessment, and decision-support systems in a production environment.

---
"""

